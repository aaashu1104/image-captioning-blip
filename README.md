# ğŸ§  Image Captioning with Visual Attention

This project generates natural language captions for images using an encoder-decoder architecture with Bahdanau Attention â€” inspired by the paper *"Show, Attend and Tell"*.

## ğŸ“¸ Example Output
**Input Image:** (Upload an image in Colab)  
**Generated Caption:** â€œA group of people standing in a kitchen preparing food.â€

---

## ğŸš€ Key Features
- ğŸ¯ **Encoder**: InceptionResNetV2 for extracting image features
- âœï¸ **Decoder**: GRU-based with attention mechanism
- ğŸ“ **Dataset**: MS-COCO 2014 captions dataset
- ğŸ“ Trained on Google Colab (GPU accelerated)
- ğŸ“¤ Custom image upload and caption generation

---

## ğŸ› ï¸ How to Run

1. **Open in Colab**  
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Ndxq_xWgtY-ztiKAmeRLw4-WX2S6fg3x?usp=sharing)

2. **Upload your own image** in the Colab interface

3. **Generate Captions** with the trained model

---

## ğŸ§± Model Architecture

```text
Encoder (InceptionResNetV2)
    â†“
Feature Vector (64x2048)
    â†“
Bahdanau Attention
    â†“
GRU Decoder
    â†“
Caption Output
